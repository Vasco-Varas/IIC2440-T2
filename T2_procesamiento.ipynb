{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T21:22:19.103438Z",
     "start_time": "2024-06-24T21:22:18.975056Z"
    },
    "id": "SS2NYAx7Vgfs"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "mecLubDGms4y",
    "outputId": "f488653a-347b-43e5-a90b-7e4940898b64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/18 22:41:35 WARN Utils: Your hostname, vserver resolves to a loopback address: 127.0.1.1; using 192.168.1.97 instead (on interface enp6s18)\n",
      "24/06/18 22:41:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/18 22:41:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.97:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TriangleFinding</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=TriangleFinding>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import random\n",
    "\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "\n",
    "# Configuración de Spark\n",
    "conf = SparkConf().setAppName(\"TriangleFinding\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Ejemplo de aristas del grafo como una lista de tuplas\n",
    "edges = [\n",
    "    (1, '11', 2),\n",
    "    (2, '11', 3),\n",
    "    (3, '11', 1),\n",
    "    (3, '11', 4),\n",
    "    (4, '11', 5),\n",
    "    (5, '11', 3),\n",
    "    # Agrega más aristas según sea necesario\n",
    "]\n",
    "\n",
    "# Crear un RDD a partir de la lista de aristas\n",
    "edges_rdd = sc.parallelize(edges)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0sbcPPWCdg5u"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,11,2),(2,12,3),(3,13,1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAe2LW0clREs"
   },
   "source": [
    "Voy a empezar creando la función de hash. Entre algunas cosas que asumimos está que un nodo no puede tener una arista consigo mismo, entonces el número de nodos tiene que ver con el numero de aristas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0e3bxMMlT5z",
    "outputId": "8c6adead-4fc2-419d-f009-f93bb0d02c31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1He92FA3oQ-X",
    "outputId": "bfd750a5-9db9-4401-a8c7-d9f90b775745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_obtener_nodos(rdd):\n",
    "  nodos = []\n",
    "  for elemento in rdd.collect():\n",
    "    if elemento[0] not in nodos:\n",
    "      nodos.append(elemento[0])\n",
    "\n",
    "    if elemento[2] not in nodos:\n",
    "      nodos.append(elemento[2])\n",
    "  return nodos\n",
    "func_obtener_nodos(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5KLDqPYRWv9"
   },
   "source": [
    "Implementa un programa en PySpark que entregue todos los tri´angulos (como tuplas de tres nodos)\n",
    "en el grafo usando b\n",
    "3\n",
    "reducers, donde b es un par´ametro. Para esta primera parte puedes asumir que\n",
    "tu grafo solo usa una etiqueta de arista (en el grafo de prueba, esa etiqueta corresponde al numero 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNqoWzgRnoVp",
    "outputId": "9c4a1a25-bb03-4042-a4dc-af9cda78f1cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 1), (2, 3, 1), (3, 1, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (x[0], x[2], 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNSX5_3ZsqE-",
    "outputId": "a13f1fe7-1c0a-4efb-8f12-74bd8dc2b054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 3), (2, 3, 1), (3, 1, 2), (3, 4, 5), (4, 5, 3), (5, 3, 4)]\n"
     ]
    }
   ],
   "source": [
    "def hash_node(v, b):\n",
    "  # Hasheamos los nodos, ej b_1 = h(n_1)\n",
    "  return hash(v) % b\n",
    "\n",
    "# Fase de mapeo\n",
    "def map_triangles(arista):\n",
    "  n_1, R, n_2 = arista\n",
    "  b_1 = hash_node(n_1, b)\n",
    "\n",
    "  b_2 = hash_node(n_2, b)\n",
    "\n",
    "  resultado = []\n",
    "  for b_3 in range(b):\n",
    "    resultado.append(((b_1, b_2, b_3), (n_1, R, n_2)))\n",
    "    resultado.append(((b_1, b_3, b_2), (n_1, R, n_2)))\n",
    "    resultado.append(((b_3, b_1, b_2), (n_1, R, n_2)))\n",
    "\n",
    "  return resultado\n",
    "\n",
    "\n",
    "\n",
    "# Fase de Reduce\n",
    "def reduce_triangles(llave, aristas):\n",
    "  triangles = []\n",
    "\n",
    "  aristas_dict = dict()\n",
    "\n",
    "  for arista in aristas:\n",
    "\n",
    "    n_1, R, n_2 = arista\n",
    "    if n_1 not in aristas_dict:\n",
    "      aristas_dict[n_1] = []\n",
    "    aristas_dict[n_1].append((R, n_2))\n",
    "\n",
    "  for n_1 in aristas_dict:\n",
    "    for (R, n_2) in aristas_dict[n_1]:\n",
    "      if n_2 in aristas_dict:\n",
    "        for (s, n_3) in aristas_dict[n_2]:\n",
    "          if n_3 in aristas_dict and any(n_4 == n_1 for (_, n_4) in aristas_dict[n_3]):\n",
    "            triangles.append((n_1, n_2, n_3))\n",
    "  return triangles\n",
    "\n",
    "# Parámetro b\n",
    "b = 2\n",
    "\n",
    "# Fase de Map\n",
    "mapped_rdd = edges_rdd.flatMap(lambda edge: map_triangles(edge))\n",
    "\n",
    "# Particionamos la rdd para que se usen exactamente b^3 reducers\n",
    "\n",
    "partitioned_rdd = mapped_rdd.partitionBy(b**3)\n",
    "\n",
    "grouped_rdd = mapped_rdd.groupByKey()\n",
    "\n",
    "triangles_rdd = grouped_rdd.flatMap(lambda key_value: reduce_triangles(key_value[0], list(key_value[1])))\n",
    "\n",
    "# Acción final para recolectar los resultados\n",
    "triangles = triangles_rdd.collect()\n",
    "\n",
    "# Mostrar los triángulos encontrados\n",
    "print(triangles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6u5ypbCXu7Gw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMqvHqy1RlBr"
   },
   "source": [
    "El enunciado no especifica si la matriz de relaciones es un pandas dataframe o una lista de listas, por lo que mi grupo asumió lo segundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C4yn5raR90Y"
   },
   "source": [
    "Asume ahora que recibes un subgrafo como tres arreglos: un arreglo A con las variables, otro L con los\n",
    "tipos de aristas, y una matriz M de tama˜no |A| × |L| × |A| que tiene un uno en la posicion (x, R, y) si\n",
    "y solo si (x, R, y) es una arista de tu subgrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1m4OH14CR-pI",
    "outputId": "b3b3e398-0a7a-416f-b47c-6cd709632411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aristas:  [(0, 'R', 1), (0, 'S', 2), (1, 'S', 1), (1, 'T', 2), (2, 'S', 0), (2, 'T', 1), (3, 'R', 2), (3, 'S', 1)]\n",
      "[(1, 1, 1), (1, 1, 2), (1, 2, 1), (2, 1, 1), (1, 1, 1), (0, 1, 2), (1, 2, 0), (2, 0, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1), (1, 1, 1)]\n"
     ]
    }
   ],
   "source": [
    "A = [0, 1, 2, 3] #Reemplazar por el arreglo de variables\n",
    "L = [\"R\", \"S\", \"T\"] #Reemplazar por el arreglo de tipos de aristas\n",
    "\n",
    "M = \"\" #Reemplazar por la matriz M\n",
    "\n",
    "# Ej de M:\n",
    "M = np.array([\n",
    "    # R  S  T\n",
    "    [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 0]], # x = 0\n",
    "    [[0, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], # x = 1 # Chequear error (1,1)\n",
    "    [[0, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0]], # x = 2\n",
    "    [[0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 0]]  # x = 3\n",
    "])\n",
    "\n",
    "def matriz_a_aristas(A, L, M):\n",
    "    aristas = []\n",
    "    for x in range(len(M)):\n",
    "        for y in range(len(M[0])):\n",
    "            for r in range(len(M[0][0])):\n",
    "              if M[x][y][r] == 1:\n",
    "                  aristas.append((A[x], L[y], A[r]))\n",
    "    return aristas\n",
    "\n",
    "aristas = matriz_a_aristas(A, L, M)\n",
    "\n",
    "print(\"Aristas: \", aristas)\n",
    "\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(\"Subgrafos\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "aristas_rdd2 = sc.parallelize(aristas)\n",
    "\n",
    "\n",
    "# Ahora al igual que antes buscamos los patrones triangulares en el grafo\n",
    "# Parámetro b\n",
    "b = 2\n",
    "\n",
    "# Fase de Map\n",
    "mapped_rdd = aristas_rdd2.flatMap(lambda edge: map_triangles(edge))\n",
    "\n",
    "# Particionamos la rdd para que se usen exactamente b^3 reducers\n",
    "\n",
    "partitioned_rdd = mapped_rdd.partitionBy(b**3)\n",
    "\n",
    "grouped_rdd = mapped_rdd.groupByKey()\n",
    "\n",
    "triangles_rdd = grouped_rdd.flatMap(lambda key_value: reduce_triangles(key_value[0], list(key_value[1])))\n",
    "\n",
    "# Acción final para recolectar los resultados\n",
    "triangles = triangles_rdd.collect()\n",
    "\n",
    "# Mostrar los triángulos encontrados\n",
    "print(triangles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Spark\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "conf = SparkConf().setAppName(\"p2\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "def hash_node(node, b):\n",
    "    return node % b\n",
    "\n",
    "def mapper_func(edge, b):\n",
    "    n1, r, n2 = edge\n",
    "    h_n1 = hash_node(n1, b)\n",
    "    h_n2 = hash_node(n2, b)\n",
    "    \n",
    "    messages = []\n",
    "    \n",
    "    # (x,11,y) -> (n1, r, n2)\n",
    "    for h in range(b):\n",
    "        messages.append(((h_n1, h_n2, h, h), (n1, r, n2)))\n",
    "    \n",
    "    # (y,11,z) -> (n1, r, n2)\n",
    "    for h in range(b):\n",
    "        messages.append(((h, h_n1, h_n2, h), (n1, r, n2)))\n",
    "    \n",
    "    # (z,11,w) -> (n1, r, n2)\n",
    "    for h in range(b):\n",
    "        messages.append(((h, h, h_n1, h_n2), (n1, r, n2)))\n",
    "    \n",
    "    # (w,11,x) -> (n1, r, n2)\n",
    "    for h in range(b):\n",
    "        messages.append(((h_n2, h, h, h_n1), (n1, r, n2)))\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def reducer_func(key, edges):\n",
    "    matches = []\n",
    "    edges = list(edges)\n",
    "    \n",
    "    edge_dict = {}\n",
    "    for n1, r, n2 in edges:\n",
    "        if n1 not in edge_dict:\n",
    "            edge_dict[n1] = []\n",
    "        edge_dict[n1].append((r, n2))\n",
    "    \n",
    "    # Find patterns\n",
    "    for n1, r1, n2 in edges:\n",
    "        if n2 in edge_dict:\n",
    "            for r2, n3 in edge_dict[n2]:\n",
    "                if n3 in edge_dict:\n",
    "                    for r3, n4 in edge_dict[n3]:\n",
    "                        if n4 in edge_dict and any(r4 == 11 and n1_ == n1 for r4, n1_ in edge_dict[n4]):\n",
    "                            matches.append((n1, n2, n3, n4))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# The example graph edges given on the .md\n",
    "graph_edges = [\n",
    "    (1, 11, 2), (1, 11, 3), (2, 11, 3), (3, 11, 2),\n",
    "    (3, 11, 4), (4, 11, 1), (4, 11, 2), (4, 11, 3),\n",
    "    (4, 12, 5), (5, 12, 1), (5, 12, 2), (5, 12, 6)\n",
    "]\n",
    "\n",
    "b = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following matches were found\n",
      "-------\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "(1, 2, 3, 4)\n",
      "(2, 3, 2, 3)\n",
      "(2, 3, 4, 1)\n",
      "(2, 3, 4, 3)\n",
      "(3, 2, 3, 2)\n",
      "(3, 2, 3, 4)\n",
      "(3, 4, 1, 2)\n",
      "(3, 4, 3, 2)\n",
      "(3, 4, 3, 4)\n",
      "(3, 4, 5, 2)\n",
      "(4, 1, 2, 3)\n",
      "(4, 3, 2, 3)\n",
      "(4, 3, 4, 3)\n",
      "(4, 5, 2, 3)\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "edges_rdd = sc.parallelize(graph_edges)\n",
    "mapped_rdd = edges_rdd.flatMap(lambda edge: mapper_func(edge, b))\n",
    "reduced_rdd = mapped_rdd.groupByKey().flatMap(lambda kv: reducer_func(kv[0], kv[1]))\n",
    "matches = reduced_rdd.collect()\n",
    "\n",
    "print('The following matches were found')\n",
    "print('-------')\n",
    "for match in matches:\n",
    "    print(match)\n",
    "print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
